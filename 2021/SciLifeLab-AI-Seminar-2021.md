SciLifeLab AI Seminar Series
## A sneak peek into the future of AI-assisted life science
Wei Ouyang, Anthony Cesnik

KTH | SciLifeLab, Stockholm

-----
<!-- .slide: data-background="white" -->
## The trending NLP models
The exploding Natural Language Processing (NLP) model size
![](https://miro.medium.com/max/582/1*C-KNWQC_wXh-Q2wc6VPK1g.png)

-----
## GPT-3 -- the most powerful AI in the world

* Made by OpenAI, released in June 2020
* 175 billion parameters
* Trained on 45TB of text data
* Take 355 years to train GPT-3 on a Tesla V100
* Costs $12 million to train once

(Brown et. al, 2020)
-----
<iframe style="width:100%;height:100vh" src="https://jalammar.github.io/how-gpt3-works-visualizations-animations/"></iframe>

-----
## Codex model for code generation
 * OpenAI Codex is a descendant of GPT-3
 * Trained on billions of lines of source code
 * Made for code generation in **Python**, Go, JavaScript, Perl, PHP, Ruby, Shell, Swift, and TypeScript etc.

(Chen et. al 2021)
-----
## Key Concepts: Autocompletion & few-shot learning
<!-- .slide: data-background="white" -->
<img src="https://raw.githubusercontent.com/oeway/slides/master/2021/robot-autocompletion-example.png">
<img src="https://miro.medium.com/max/1000/1*q-P5aQ7A6VlsfroP3ckg8A.jpeg">

-----
# üî•Codex live demos!
## Disclaimer!
 * All the demo applications have not yet been approved for launch
 * Please don't record the demos

-----
## Codex basic demos
 * Explain code
 * Write a Python docstring
 * Python bug fixer

-----
## Generate code for life science

Design a prompt for data anlysis by "show and tell":

 * Provide enough context information
    - describe the overall setting and goal
    - describe preferred tools/libraries
    - provide example code if necessary
 * Create a respond pattern

-----
# üî•More live demos!
## Disclaimer!
 * All the demo applications have not yet been approved for launch
 * Please don't record the demos

-----
# Why Codex is important?

-----
## Make scientific software easier to use
Usibility & Flexiblity, we can have both!

<img src="https://docs.google.com/drawings/d/e/2PACX-1vScQuz7AGaLzhsTCvnnA_ulKUCxq0Xj6M72uBTC9Rn5-b_0FudVSemDfW4a6_WR8Gl_i4mzJbQ6GM8m/pub?w=1218&amp;h=646">

-----
## Generalization and Interpretability
 * Deep neural network has generalization issue
    - Example: Addition of two numbers
 * The result of DNN is hard to interpret and explain

-----
## Using existing knowledge and tools

Codex provide a hybrid solution for combining:
 * Analytical solution backed by math
 * Deep neural networks
 
-----
## Other advantages of Codex
 * Zero/Few-shot learning for low data domain
 * Building trust between human & AI

-----
## Towards a future of AI-assisted life science
 * Literature mining
 * Format unstructured data
 * Data-driven hyphothesis generation
 * Generate code for everyting!
     - analyze data
     - interactive data exploration
     - control instrument
     - create reports

-----
## Discussion
 * Limitations and criticisms
     * Is it true intelligence?
     * What we asked vs what we wanted
 * Ethical issues
 
 [On the Opportunities and Risks of Foundation Models](https://fsi.stanford.edu/publication/opportunities-and-risks-foundation-models)

 "GPT-4 will have 100 trillion parameters!" <!-- .element: class="fragment" data-fragment-index="1" -->

-----
### Acknowledgements

Work carried out at Cell Profiling group @ SciLifeLab headed by Emma Lundberg

We thank OpenAI for providing beta testing access to GPT-3 and Codex

The demos during this talk has not been approved by OpenAI

-----

# üôèThank You!


